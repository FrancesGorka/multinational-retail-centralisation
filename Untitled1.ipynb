{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48a49777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import tabula as tb\n",
    "import requests\n",
    "import boto3\n",
    "import io\n",
    "import yaml\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "class DatabaseConnector:\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_db_creds(creds):\n",
    "        with open(creds, 'r') as f:\n",
    "            yaml_data = yaml.safe_load(f)\n",
    "        return yaml_data\n",
    "\n",
    "    @staticmethod\n",
    "    def init_db_engine(creds):\n",
    "        credentials = DatabaseConnector.read_db_creds(creds)\n",
    "\n",
    "        HOST = credentials['HOST']\n",
    "        PASSWORD = credentials['PASSWORD']\n",
    "        USER = credentials['USER']\n",
    "        DATABASE = credentials['DATABASE']\n",
    "        PORT = credentials['PORT']\n",
    "\n",
    "        connection_string = f\"postgresql+psycopg2://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\"\n",
    "        engine = create_engine(connection_string)\n",
    "\n",
    "        return engine\n",
    "    \n",
    "    def list_db_tables(self):\n",
    "        engine = self.init_db_engine(\"db_creds.yaml\")\n",
    "        tables = []\n",
    "        with engine.connect() as connection:\n",
    "            query = text(\"SELECT tablename FROM pg_catalog.pg_tables WHERE schemaname = 'public';\")\n",
    "            result = connection.execute(query)\n",
    "\n",
    "        for row in result:\n",
    "            tables.append(row[0])\n",
    "\n",
    "        return tables\n",
    "\n",
    "    def upload_to_db(self, df, table_name):\n",
    "        try:\n",
    "            creds = \"sales_db_creds.yaml\"\n",
    "            engine = self.init_db_engine(creds)\n",
    "            df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "            print(f\"Table '{table_name}' has been created in the database.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating table '{table_name}': {str(e)}\")\n",
    "\n",
    "class DataExtractor:\n",
    "    def __init__(self, db_connector):\n",
    "        self.db_connector = db_connector\n",
    "\n",
    "    def read_rds_table(self, table_name, creds):\n",
    "        tables = self.db_connector.list_db_tables()\n",
    "        if table_name in tables:\n",
    "            engine = self.db_connector.init_db_engine(creds)\n",
    "            dataframe = pd.read_sql(table_name, engine)\n",
    "            return dataframe\n",
    "        else:\n",
    "            print(\"Table not found.\")\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def retrieve_pdf_data(url):\n",
    "        all_pages_df = tb.read_pdf(url, pages='all')\n",
    "        combined_df = pd.concat(all_pages_df, ignore_index=True)\n",
    "        return combined_df\n",
    "\n",
    "    @staticmethod\n",
    "    def list_number_of_stores(endpoint, header_dict):\n",
    "        response = requests.get(endpoint, headers=header_dict)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()['number_stores']\n",
    "        else:\n",
    "            print(\"Failed to fetch the number of stores\")\n",
    "            return 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def retrieve_stores_data(endpoint, header_dict, no_of_stores):\n",
    "        store_data_list = []\n",
    "\n",
    "        for store in range(no_of_stores):\n",
    "            full_endpoint = endpoint.format(store)\n",
    "            response = requests.get(full_endpoint, headers=header_dict)\n",
    "            if response.status_code == 200:\n",
    "                store_data_list.append(response.json())\n",
    "            else:\n",
    "                print(f\"Failed to retrieve data for store {store}\")\n",
    "                store_data_list.append(0)\n",
    "\n",
    "        stores_df = pd.DataFrame(store_data_list)\n",
    "        return stores_df\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_from_s3(s3_address):\n",
    "        s3 = boto3.client('s3')\n",
    "        bucket, key = s3_address.replace('s3://', '').split('/', 1)\n",
    "        try:\n",
    "            response = s3.get_object(Bucket=bucket, Key=key)\n",
    "            body = response['Body'].read()\n",
    "            df = pd.read_csv(io.BytesIO(body))\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract data from S3: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "class DataCleaning:\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_invalid_date(df, column_name):\n",
    "        df[column_name] = pd.to_datetime(df[column_name], format='%Y-%m-%d', errors='ignore')\n",
    "        df[column_name] = pd.to_datetime(df[column_name], format='%Y %B %d', errors='ignore')\n",
    "        df[column_name] = pd.to_datetime(df[column_name], format='%B %Y %d', errors='ignore')\n",
    "        df[column_name] = pd.to_datetime(df[column_name], errors='coerce')\n",
    "        df.dropna(subset=[column_name], how='any', inplace=True)\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_user_data(user_data):\n",
    "        user_data = DataCleaning.clean_invalid_date(user_data, 'date_of_birth')\n",
    "        user_data = DataCleaning.clean_invalid_date(user_data, 'join_date')\n",
    "        user_data['address'] = user_data['address'].str.replace('\\n', ', ')\n",
    "        user_data = user_data[user_data['email_address'].str.match(r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$')]\n",
    "        user_data.loc[:, 'phone_number'] = user_data['phone_number'].str.replace(r'\\D', '', regex=True)\n",
    "        return user_data\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_card_data(card_data):\n",
    "        card_data = DataExtractor.retrieve_pdf_data(card_data)\n",
    "        card_data['card_number'] = card_data['card_number'].apply(str)\n",
    "        card_data = DataCleaning.clean_invalid_date(card_data, 'date_payment_confirmed')\n",
    "        card_data = DataCleaning.clean_invalid_date(card_data, 'expiry_date')\n",
    "        return card_data\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_continent(store_df):\n",
    "        store_df['continent'] = store_df['continent'].apply(lambda continent: \"Europe\" if \"Europe\" in continent else (\"America\" if \"America\" in continent else None))\n",
    "        store_df.dropna(subset=['continent'], inplace=True)\n",
    "        return store_df\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_store_data(store_df):\n",
    "        store_df.drop(columns='lat', inplace=True)\n",
    "        store_df = DataCleaning.clean_invalid_date(store_df, \"opening_date\")\n",
    "        store_df['address'] = store_df['address'].str.replace('\\n', ', ')\n",
    "        store_df = DataCleaning.clean_continent(store_df)\n",
    "        return store_df\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_multiple_weights(products):\n",
    "        pattern = r'(\\d+(\\.\\d+)?)\\s*([kKgGmMoOzZlL]*)'\n",
    "        products['weight'] = products['weight'].astype(str)\n",
    "        for i, product in products.iterrows():\n",
    "            if \"x\" in product['weight']:\n",
    "                matches = re.findall(pattern, product['weight'])\n",
    "                new_weights = []\n",
    "                for match in matches:\n",
    "                    quantity = float(match[0])\n",
    "                    unit_weight = float(match[1]) if match[1] else 1.0\n",
    "                    unit = match[2].lower()\n",
    "                    new_weight = quantity * unit_weight\n",
    "                    new_weights.append(f\"{new_weight:.2f}{unit}\")\n",
    "                products.at[i, 'weight'] = ' x '.join(new_weights)\n",
    "        return products\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_product_weights(products):\n",
    "        products = DataCleaning.convert_multiple_weights(products)\n",
    "        for index, row in products.iterrows():\n",
    "            weight = str(row['weight'])\n",
    "            if \"kg\" in weight:\n",
    "                numeric_part = weight.replace(\"kg\", \"\").strip()\n",
    "                numeric_value = float(numeric_part)\n",
    "                products.at[index, 'weight'] = numeric_value\n",
    "            elif \"oz\" in weight:\n",
    "                numeric_part = weight.replace(\"oz\", \"\").strip()\n",
    "                numeric_value = float(numeric_part)\n",
    "                products.at[index, 'weight'] = numeric_value * 0.0283495\n",
    "            elif \"ml\" in weight:\n",
    "                numeric_part = weight.replace(\"ml\", \"\").strip()\n",
    "                numeric_value = float(numeric_part)\n",
    "                products.at[index, 'weight'] = numeric_value * 1.28\n",
    "            elif \"g\" in weight:\n",
    "                numeric_part = weight.replace(\"g\", \"\").strip()\n",
    "                numeric_value = float(numeric_part)\n",
    "                products.at[index, 'weight'] = numeric_value / 1000\n",
    "        return products\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_products_data(products):\n",
    "        products.dropna(inplace=True)\n",
    "        products = DataCleaning.convert_product_weights(products)\n",
    "        products = DataCleaning.clean_invalid_date(products, 'date_added')\n",
    "        return products\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_orders_data(orders_table):\n",
    "        columns_to_drop = ['first_name', 'last_name', '1']\n",
    "        orders_table.drop(columns=columns_to_drop, inplace=True)\n",
    "        return orders_table\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_sales_data(sales_table):\n",
    "        sales_table['timestamp'] = pd.to_datetime(sales_table['timestamp'], format='%H:%M:%S')\n",
    "        return sales_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d26eb8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_79105/345551987.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  user_data.loc[:, 'phone_number'] = user_data['phone_number'].str.replace(r'\\D', '', regex=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'dim_users' has been created in the database.\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
