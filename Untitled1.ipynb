{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48a49777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import tabula as tb\n",
    "import requests\n",
    "import boto3\n",
    "import io\n",
    "import yaml\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "class DatabaseConnector:\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_db_creds(creds):\n",
    "        with open(creds, 'r') as f:\n",
    "            yaml_data = yaml.safe_load(f)\n",
    "        return yaml_data\n",
    "\n",
    "    @staticmethod\n",
    "    def init_db_engine(creds):\n",
    "        credentials = DatabaseConnector.read_db_creds(creds)\n",
    "\n",
    "        HOST = credentials['HOST']\n",
    "        PASSWORD = credentials['PASSWORD']\n",
    "        USER = credentials['USER']\n",
    "        DATABASE = credentials['DATABASE']\n",
    "        PORT = credentials['PORT']\n",
    "\n",
    "        connection_string = f\"postgresql+psycopg2://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\"\n",
    "        engine = create_engine(connection_string)\n",
    "\n",
    "        return engine\n",
    "    \n",
    "    def list_db_tables(self):\n",
    "        engine = self.init_db_engine(\"db_creds.yaml\")\n",
    "        tables = []\n",
    "        with engine.connect() as connection:\n",
    "            query = text(\"SELECT tablename FROM pg_catalog.pg_tables WHERE schemaname = 'public';\")\n",
    "            result = connection.execute(query)\n",
    "\n",
    "        for row in result:\n",
    "            tables.append(row[0])\n",
    "\n",
    "        return tables\n",
    "\n",
    "    def upload_to_db(self, df, table_name):\n",
    "        try:\n",
    "            creds = \"sales_db_creds.yaml\"\n",
    "            engine = self.init_db_engine(creds)\n",
    "            df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "            print(f\"Table '{table_name}' has been created in the database.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating table '{table_name}': {str(e)}\")\n",
    "\n",
    "class DataExtractor:\n",
    "    def __init__(self, db_connector):\n",
    "        self.db_connector = db_connector\n",
    "\n",
    "    def read_rds_table(self, table_name, creds):\n",
    "        tables = self.db_connector.list_db_tables()\n",
    "        if table_name in tables:\n",
    "            engine = self.db_connector.init_db_engine(creds)\n",
    "            dataframe = pd.read_sql(table_name, engine)\n",
    "            return dataframe\n",
    "        else:\n",
    "            print(\"Table not found.\")\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def retrieve_pdf_data(url):\n",
    "        all_pages_df = tb.read_pdf(url, pages='all')\n",
    "        combined_df = pd.concat(all_pages_df, ignore_index=True)\n",
    "        return combined_df\n",
    "\n",
    "    @staticmethod\n",
    "    def list_number_of_stores(endpoint, header_dict):\n",
    "        response = requests.get(endpoint, headers=header_dict)\n",
    "        if response.status_code == 200:\n",
    "            return int(response.json())\n",
    "        else:\n",
    "            print(\"Failed to fetch the number of stores\")\n",
    "            return 0\n",
    "\n",
    "    @staticmethod\n",
    "    def retrieve_stores_data(endpoint, header_dict, no_of_stores):\n",
    "        store_data_list = []\n",
    "\n",
    "        for store in range(no_of_stores):\n",
    "            full_endpoint = endpoint.format(store)\n",
    "            response = requests.get(full_endpoint, headers=header_dict)\n",
    "            if response.status_code == 200:\n",
    "                store_data_list.append(response.json())\n",
    "            else:\n",
    "                print(f\"Failed to retrieve data for store {store}\")\n",
    "                store_data_list.append(0)\n",
    "\n",
    "        stores_df = pd.DataFrame(store_data_list)\n",
    "        return stores_df\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_from_s3(s3_address):\n",
    "        s3 = boto3.client('s3')\n",
    "        bucket, key = s3_address.replace('s3://', '').split('/', 1)\n",
    "        try:\n",
    "            response = s3.get_object(Bucket=bucket, Key=key)\n",
    "            body = response['Body'].read()\n",
    "            df = pd.read_csv(io.BytesIO(body))\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract data from S3: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "class DataCleaning:\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_invalid_date(df, column_name):\n",
    "        df[column_name] = pd.to_datetime(df[column_name], format='%Y-%m-%d', errors='ignore')\n",
    "        df[column_name] = pd.to_datetime(df[column_name], format='%Y %B %d', errors='ignore')\n",
    "        df[column_name] = pd.to_datetime(df[column_name], format='%B %Y %d', errors='ignore')\n",
    "        df[column_name] = pd.to_datetime(df[column_name], errors='coerce')\n",
    "        df.dropna(subset=[column_name], how='any', inplace=True)\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_user_data(user_data):\n",
    "        user_data = DataCleaning.clean_invalid_date(user_data, 'date_of_birth')\n",
    "        user_data = DataCleaning.clean_invalid_date(user_data, 'join_date')\n",
    "        user_data['address'] = user_data['address'].str.replace('\\n', ', ')\n",
    "        user_data = user_data[user_data['email_address'].str.match(r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$')]\n",
    "        user_data.loc[:, 'phone_number'] = user_data['phone_number'].str.replace(r'\\D', '', regex=True)\n",
    "        return user_data\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_card_data(card_data):\n",
    "        card_data = DataExtractor.retrieve_pdf_data(card_data)\n",
    "        card_data['card_number'] = card_data['card_number'].apply(str)\n",
    "        card_data = DataCleaning.clean_invalid_date(card_data, 'date_payment_confirmed')\n",
    "        card_data = DataCleaning.clean_invalid_date(card_data, 'expiry_date')\n",
    "        return card_data\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_continent(store_df):\n",
    "        store_df['continent'] = store_df['continent'].apply(lambda continent: \"Europe\" if \"Europe\" in continent else (\"America\" if \"America\" in continent else None))\n",
    "        store_df.dropna(subset=['continent'], inplace=True)\n",
    "        return store_df\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_store_data(store_df):\n",
    "        store_df.drop(columns='lat', inplace=True)\n",
    "        store_df = DataCleaning.clean_invalid_date(store_df, \"opening_date\")\n",
    "        store_df['address'] = store_df['address'].str.replace('\\n', ', ')\n",
    "        store_df = DataCleaning.clean_continent(store_df)\n",
    "        return store_df\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_multiple_weights(products):\n",
    "        pattern = r'(\\d+(\\.\\d+)?)\\s*([kKgGmMoOzZlL]*)'\n",
    "        products['weight'] = products['weight'].astype(str)\n",
    "        for i, product in products.iterrows():\n",
    "            if \"x\" in product['weight']:\n",
    "                matches = re.findall(pattern, product['weight'])\n",
    "                new_weights = []\n",
    "                for match in matches:\n",
    "                    quantity = float(match[0])\n",
    "                    unit_weight = float(match[1]) if match[1] else 1.0\n",
    "                    unit = match[2].lower()\n",
    "                    new_weight = quantity * unit_weight\n",
    "                    new_weights.append(f\"{new_weight:.2f}{unit}\")\n",
    "                products.at[i, 'weight'] = ' x '.join(new_weights)\n",
    "        return products\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_product_weights(products):\n",
    "        products = DataCleaning.convert_multiple_weights(products)\n",
    "        for index, row in products.iterrows():\n",
    "            weight = str(row['weight'])\n",
    "            if \"kg\" in weight:\n",
    "                numeric_part = weight.replace(\"kg\", \"\").strip()\n",
    "                numeric_value = float(numeric_part)\n",
    "                products.at[index, 'weight'] = numeric_value\n",
    "            elif \"oz\" in weight:\n",
    "                numeric_part = weight.replace(\"oz\", \"\").strip()\n",
    "                numeric_value = float(numeric_part)\n",
    "                products.at[index, 'weight'] = numeric_value * 0.0283495\n",
    "            elif \"ml\" in weight:\n",
    "                numeric_part = weight.replace(\"ml\", \"\").strip()\n",
    "                numeric_value = float(numeric_part)\n",
    "                products.at[index, 'weight'] = numeric_value * 1.28\n",
    "            elif \"g\" in weight:\n",
    "                numeric_part = weight.replace(\"g\", \"\").strip()\n",
    "                numeric_value = float(numeric_part)\n",
    "                products.at[index, 'weight'] = numeric_value / 1000\n",
    "        return products\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_products_data(products):\n",
    "        products.dropna(inplace=True)\n",
    "        products = DataCleaning.convert_product_weights(products)\n",
    "        products = DataCleaning.clean_invalid_date(products, 'date_added')\n",
    "        return products\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_orders_data(orders_table):\n",
    "        columns_to_drop = ['first_name', 'last_name', '1']\n",
    "        orders_table.drop(columns=columns_to_drop, inplace=True)\n",
    "        return orders_table\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_sales_data(sales_table):\n",
    "        sales_table['timestamp'] = pd.to_datetime(sales_table['timestamp'], format='%H:%M:%S')\n",
    "        return sales_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d26eb8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_79105/927908059.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  user_data.loc[:, 'phone_number'] = user_data['phone_number'].str.replace(r'\\D', '', regex=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'dim_users' has been created in the database.\n"
     ]
    }
   ],
   "source": [
    "# Initialize database connector, extractor, and cleaner\n",
    "db_connector = DatabaseConnector()\n",
    "db_extractor = DataExtractor(db_connector)  # Pass the db_connector instance to DataExtractor\n",
    "db_cleaner = DataCleaning()\n",
    "\n",
    "# Define API endpoints and headers\n",
    "no_of_stores_endpoint = \"https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/number_stores\"\n",
    "store_retrieval_endpoint = \"https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/store_details/{store_number}\"\n",
    "header_dict = {'x-api-key': 'yFBQbwXe9J3sd6zWVAMrK6lcxxr0q1lr2PT6DDMX'}\n",
    "\n",
    "# Define data sources\n",
    "card_data_link = \"https://data-handling-public.s3.eu-west-1.amazonaws.com/card_details.pdf\"\n",
    "s3_products = \"s3://data-handling-public/products.csv\"\n",
    "s3_sales = \"https://data-handling-public.s3.eu-west-1.amazonaws.com/date_details.json\"\n",
    "\n",
    "# Extract and clean user data\n",
    "user_table = db_extractor.read_rds_table('legacy_users','db_creds.yaml')\n",
    "cleaned_user_data = db_cleaner.clean_user_data(user_table)\n",
    "db_connector.upload_to_db(cleaned_user_data, 'dim_users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88703373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'dim_card_details' has been created in the database.\n"
     ]
    }
   ],
   "source": [
    "# Extract and clean card data\n",
    "cleaned_card_data = db_cleaner.clean_card_data(card_data_link)\n",
    "db_connector.upload_to_db(cleaned_card_data,'dim_card_details')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c94439a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Extract and clean store data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m no_of_stores \u001b[38;5;241m=\u001b[39m db_extractor\u001b[38;5;241m.\u001b[39mlist_number_of_stores(no_of_stores_endpoint, header_dict)\n\u001b[0;32m----> 3\u001b[0m store_data \u001b[38;5;241m=\u001b[39m \u001b[43mdb_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve_stores_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore_retrieval_endpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_of_stores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m cleaned_store_data \u001b[38;5;241m=\u001b[39m db_cleaner\u001b[38;5;241m.\u001b[39mclean_store_data(store_data)\n\u001b[1;32m      5\u001b[0m db_connector\u001b[38;5;241m.\u001b[39mupload_to_db(cleaned_store_data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdim_store_details\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 88\u001b[0m, in \u001b[0;36mDataExtractor.retrieve_stores_data\u001b[0;34m(endpoint, header_dict, no_of_stores)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve_stores_data\u001b[39m(endpoint, header_dict, no_of_stores):\n\u001b[1;32m     86\u001b[0m     store_data_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m store \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mno_of_stores\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     89\u001b[0m         full_endpoint \u001b[38;5;241m=\u001b[39m endpoint\u001b[38;5;241m.\u001b[39mformat(store)\n\u001b[1;32m     90\u001b[0m         response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(full_endpoint, headers\u001b[38;5;241m=\u001b[39mheader_dict)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "# Extract and clean store data\n",
    "no_of_stores = db_extractor.list_number_of_stores(no_of_stores_endpoint, header_dict)\n",
    "store_data = db_extractor.retrieve_stores_data(store_retrieval_endpoint, header_dict, no_of_stores)\n",
    "cleaned_store_data = db_cleaner.clean_store_data(store_data)\n",
    "db_connector.upload_to_db(cleaned_store_data, 'dim_store_details')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03ec6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract and clean products data\n",
    "products_data = db_extractor.extract_from_s3(s3_products)\n",
    "cleaned_products_data = db_cleaner.clean_products_data(products_data)\n",
    "db_connector.upload_to_db(cleaned_products_data, 'dim_products')\n",
    "\n",
    "# Extract and clean orders data\n",
    "orders_table = db_extractor.read_rds_table(db_connector, 'orders_table')\n",
    "cleaned_orders_table = db_cleaner.clean_orders_data(orders_table)\n",
    "db_connector.upload_to_db(cleaned_orders_table, 'dim_orders')\n",
    "\n",
    "# Extract and clean sales data\n",
    "sales_data = db_extractor.extract_from_s3(s3_sales)\n",
    "cleaned_sales_data = db_cleaner.clean_sales_data(sales_data)\n",
    "db_connector.upload_to_db(cleaned_sales_data, 'dim_date_times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e914ef12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Forbidden'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(\"https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/number_stores\", {'x-api-key': 'yFBQbwXe9J3sd6zWVAMrK6lcxxr0q1lr2PT6DDMX'})\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374749d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
